{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--lr LR] [--decay DECAY]\n",
      "                             [--lamb LAMB] [--lamb2 LAMB2]\n",
      "                             [--schedule SCHEDULE [SCHEDULE ...]]\n",
      "                             [--gammas GAMMAS [GAMMAS ...]] [--seed SEED]\n",
      "                             [--nepochs NEPOCHS] [--tasknum TASKNUM]\n",
      "                             [--date DATE] [--output OUTPUT]\n",
      "                             [--dataset {CIFAR100,MNIST,CIFAR10,CIFAR100_for_Resnet}]\n",
      "                             [--trainer {ewc,interpolate_pretrain,interpolate_random,interpolate_pretrain_fix_var,vanilla_basin_constraint,vanilla_basin_immediate_constraint,vanilla_middle_from_center}]\n",
      "                             [--model {resnet18,MLP}] [--optimizer {Adam,SGD}]\n",
      "                             [--grad-tilt GRAD_TILT]\n",
      "                             [--knowledge-ratio KNOWLEDGE_RATIO]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"321257fc-6921-4e6d-9636-67eced959f34\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=/tmp/tmp-91040rTqvz5hpC5s.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys, os, time\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import random\n",
    "import utils\n",
    "\n",
    "import data_handler\n",
    "from sklearn.utils import shuffle\n",
    "import trainer\n",
    "import networks\n",
    "import copy\n",
    "import torchvision\n",
    "\n",
    "def compute_distance(model1, model2):\n",
    "    norm_square_sum = 0\n",
    "    for module1, module2 in zip(model1.modules(), model2.modules()):\n",
    "        if 'Conv' in str(type(module1)):\n",
    "            norm_square_sum += torch.norm(module1.weight.data-module2.weight.data)**2\n",
    "            if module1.bias is not None: \n",
    "                norm_square_sum += torch.norm(module1.bias.data-module2.bias.data)**2\n",
    "        elif 'BatchNorm' in str(type(module1)):\n",
    "            norm_square_sum += torch.norm(module1.weight.data - module2.weight.data)**2\n",
    "            norm_square_sum += torch.norm(module1.bias.data - module2.bias.data)**2\n",
    "            #norm_square_sum += torch.norm(module1.running_mean - module2.running_mean)**2\n",
    "            #norm_square_sum += torch.norm(module1.running_var - module2.running_var)**2\n",
    "    #for (n1,p1), (n2, p2) in zip(model1.named_parameters(), model2.named_parameters()):\n",
    "    #    if 'fc' in n1 or 'last' in n1:\n",
    "    #        continue\n",
    "    #    norm_square_sum += torch.norm(p1-p2)**2\n",
    "    return torch.sqrt(norm_square_sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "def main():\n",
    "    # args = get_args()\n",
    "\n",
    "    # Seed\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = True # Control randomness\n",
    "    #device = torch.device(\"cpu\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Device:', device)\n",
    "    print('Current cuda device:', torch.cuda.current_device())\n",
    "    print('Count of using GPUs:', torch.cuda.device_count())\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print('Load data...')\n",
    "    data_dict = None\n",
    "    dataset = data_handler.DatasetFactory.get_dataset('CIFAR100_for_Resnet')\n",
    "    task_info = dataset.task_info\n",
    "    print('\\nTask info =', task_info)\n",
    "\n",
    "\n",
    "    # Loader used for training data\n",
    "    shuffle_idx = shuffle(np.arange(dataset.classes), random_state=args.seed)\n",
    "\n",
    "    # list of dataloaders: it consists of dataloaders for each task\n",
    "    train_dataset_loaders = data_handler.make_ContinualLoaders(dataset.train_data,\n",
    "                                                            dataset.train_labels,\n",
    "                                                            task_info,\n",
    "                                                            transform=dataset.train_transform,\n",
    "                                                            shuffle_idx = shuffle_idx,\n",
    "                                                            data_dict = data_dict,\n",
    "                                                           )\n",
    "\n",
    "    test_dataset_loaders = data_handler.make_ContinualLoaders(dataset.test_data,\n",
    "                                                           dataset.test_labels,\n",
    "                                                           task_info,\n",
    "                                                           transform=dataset.test_transform,\n",
    "                                                           shuffle_idx = shuffle_idx,\n",
    "                                                           data_dict = data_dict,\n",
    "                                                          )\n",
    "\n",
    "    # Get the required model\n",
    "    myModel = networks.ModelFactory.get_model(args.model, task_info).to(device)\n",
    "    test_model1 = networks.ModelFactory.get_model(args.model, task_info).to(device)\n",
    "    test_model2 = networks.ModelFactory.get_model(args.model, task_info).to(device)\n",
    "    #test_model1 = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "    #num_ftrs = test_model1.fc.in_features\n",
    "    #test_model1.fc = nn.Linear(num_ftrs, 5)\n",
    "    #test_model2 = torchvision.models.resnet18(pretrained=False).to(device)\n",
    "    #test_model2.fc = nn.Linear(num_ftrs, 5)\n",
    "\n",
    "    # Define the optimizer used in the experiment\n",
    "\n",
    "    optimizer = torch.optim.Adam(myModel.parameters(), lr=args.lr, weight_decay=args.decay)\n",
    "\n",
    "    # Initilize the evaluators used to measure the performance of the system.\n",
    "    t_classifier = trainer.EvaluatorFactory.get_evaluator(\"trainedClassifier\")\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    utils.print_model_report(myModel)\n",
    "    utils.print_optimizer_config(optimizer)\n",
    "    print('-' * 100)\n",
    "\n",
    "    interpolation_range = np.arange(-1, 2, 0.1)\n",
    "    # Loop tasks\n",
    "    acc = np.zeros((len(task_info), len(task_info), len(interpolation_range)), dtype=np.float32)\n",
    "    lss = np.zeros((len(task_info), len(task_info), len(interpolation_range)), dtype=np.float32)\n",
    "    for t, ncla in task_info:\n",
    "        if t == 0:\n",
    "            continue\n",
    "        print(\"tasknum:\", t)\n",
    "        # Add new classes to the train, and test iterator\n",
    "\n",
    "        train_loader = train_dataset_loaders[t]\n",
    "        test_loader = test_dataset_loaders[t]\n",
    "\n",
    "        for u in range(t+1):\n",
    "            test_loader = test_dataset_loaders[u]\n",
    "            test_iterator = torch.utils.data.DataLoader(test_loader, 100, shuffle=False)\n",
    "            test_model1.load_state_dict(torch.load('./trained_model/_CIFAR100_for_Resnet_from_pretrained_resnet18_SGD_0_lamb_0_lr_0.1_batch_256_epoch_100_task_0.pt'))\n",
    "            test_model2.load_state_dict(torch.load('./trained_model/_CIFAR100_for_Resnet_from_pretrained_resnet18_SGD_0_lamb_0_lr_0.1_batch_256_epoch_100_task_2.pt'))\n",
    "            distance = compute_distance(test_model1, test_model2)\n",
    "            print(\"Distance btw Two model : {}\".format(distance))\n",
    "            sys.exit()\n",
    "            for i, lamb in enumerate(interpolation_range):\n",
    "                for module, model1_module, model2_module in zip(myModel.modules(), test_model1.modules(), test_model2.modules()):\n",
    "                    if 'Conv' in str(type(module)):\n",
    "                        module.weight.data = (lamb * model1_module.weight.data + (1-lamb) * model2_module.weight.data)\n",
    "                        if module.bias is not None:\n",
    "                            module.bias.data = (lamb * model1_module.bias.data + (1-lamb) * model2_module.bias.data)\n",
    "                    elif 'BatchNorm' in str(type(module)):\n",
    "                        module.weight.data = (lamb * model1_module.weight.data + (1-lamb) * model2_module.weight.data)\n",
    "                        module.bias.data = (lamb * model1_module.bias.data + (1-lamb) * model2_module.bias.data)\n",
    "                        module.running_mean.data = (lamb * model1_module.running_mean.data + (1-lamb) * model2_module.running_mean.data)\n",
    "                        module.running_var.data = (lamb * model1_module.running_var.data + (1-lamb) * model2_module.running_var.data)\n",
    "                test_loss, test_acc = t_classifier.evaluate(myModel, test_iterator, u, device)\n",
    "                #test_loss, test_acc = t_classifier.one_task_evaluate(myModel, test_iterator, device)\n",
    "                print('>>> Test on task {:2d}: Interpolation Coefficient : {:.2f}, loss={:.3f}, acc={:5.1f}% <<<'.format(u,lamb,  test_loss, 100 * test_acc))\n",
    "                acc[t, u, i] = test_acc\n",
    "                lss[t, u, i] = test_loss\n",
    "\n",
    "\n",
    "        for task_t, acc_loss in enumerate(zip(acc, lss)):\n",
    "            acc_t, loss_t = acc_loss\n",
    "            np.savetxt('interpolate_acc_after_task_{}'.format(task_t), acc_t, '%.4f')\n",
    "        if t==0:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f5e996fd09088d65c59638b33ffd5c4fdbf1761c9b9034ce63c711ff56f8f5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
